{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdAHAK8a93DZNr9TIOeZTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2caves/LinearRegressionFromScratch/blob/main/LinearRegressionModelScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model for predicting house prices - from scratch!\n",
        "\n",
        "The purpose of this exercise is to learn about the fundamental inner-workings of a simple linear regression model, without the use of any ML libraries like scikit-learn, keras or pytorch.\n",
        "\n",
        "As this is the simplest of regression models, we will initially only be considering one feature to predict our house prices - area. Later, we will include more features from the dataset in a vectorized approach to improve the accuracy of our model predictions and further understand how to work with increasing numbers of features.\n",
        "\n",
        "We'll use stochastic gradient descent as our optimization algorithm, alongside regularization to prevent overfitting on the training data and we'll split our dataset into test and validation sets to measure how our model comparatively performs on examples it hasn't seen before.\n",
        "\n",
        "If you want to follow along, you can use this as a guide and try your own implementation separately - then compare!\n"
      ],
      "metadata": {
        "id": "fvMz3oCZno72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what's the plan?\n",
        "\n",
        "- Set up our environment with libraries we'll need\n",
        "- Grab our dataset we will use to train the model\n",
        "- Clean up the data to be useful + split into train/val/test sets\n",
        "- Define our model structure by initialization\n",
        "- Define how to train the model (predict -> loss -> gradients -> update params)\n",
        "- Plot our loss curves\n",
        "- Test our model on the test set"
      ],
      "metadata": {
        "id": "VC6Wt6DQwSrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin from scratch by importing our libraries - numpy (for efficient numeric operations), pandas (for data preprocessing) and matplotlib (for visualization)"
      ],
      "metadata": {
        "id": "E1PkRm7IqKan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nU7GbIwUqNvA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's get our dataset from Kaggle. We'll be using House Sales in King County - USA and initially only be looking at the square foot area for each example.\n",
        "\n",
        "Download the dataset here, unzip *'archive.zip'* and place the .csv contained in your *'LinearRegressionModel/datasets/'* folder."
      ],
      "metadata": {
        "id": "PiECKAhVuPpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset via Google Drive:"
      ],
      "metadata": {
        "id": "0Tu3rzm3vZw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace the 'None' for FOLDERNAME and enter the foldername in your Drive where you have saved the unzipped\n",
        "# dataset csv file, e.g. 'LinearRegressionModel/datasets/'\n",
        "FOLDERNAME = None\n",
        "\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "data = pd.read_csv('./kc_house_data.csv')\n",
        "print(f'This is a portion of our dataset:\\n{data.iloc[:4,:6]}')"
      ],
      "metadata": {
        "id": "zrnXVAVVqhrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternatively, load directly from file:\n",
        "(Be sure to upload the dataset .csv to Colab and put it in a *datasets* folder):"
      ],
      "metadata": {
        "id": "FY7YTArkvliE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('./datasets/')\n",
        "data = pd.read_csv('./datasets/kc_house_data.csv')\n",
        "\n",
        "print(f'This is a portion of our dataset:\\n{data.iloc[:4,:6]}')"
      ],
      "metadata": {
        "id": "bYbTa4oFtFGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to pre-process the data and strip it down to only the features we will be using - the price and the area in sqft.\n",
        "\n",
        "We will then load our stripped-down dataset into x and y variables for our inputs and labels respectively. For now, we'll train the model on 80% of the total examples and leave 20% to validate how our model performs after it's been trained."
      ],
      "metadata": {
        "id": "UEgS8-912H8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip dataset to columns that represent prices and areas\n",
        "data_stripped = data.iloc[:,[2,5]]\n",
        "X = data.iloc[:,[5]] # assign our model inputs as areas\n",
        "y = data.iloc[:,[2]] # assign our model labels as prices\n",
        "\n",
        "# 80/20 split for train/validation sets\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(data)*split_ratio)\n",
        "\n",
        "# Assign each split to x,y_train and x,y_val sets & convert to numpy arrays for our model.\n",
        "x_train = X.iloc[:split_index].values\n",
        "y_train = y.iloc[:split_index].values\n",
        "x_val = X.iloc[split_index:].values\n",
        "y_val = y.iloc[split_index:].values\n",
        "\n",
        "# Visualize our data splits\n",
        "print(f'x_train: {x_train}\\n')\n",
        "print(f'y_train: {y_train}\\n')\n",
        "print(f'number of original examples: {len(X)}')\n",
        "print(f'number of x_train examples: {len(x_train)} | number of x_val examples: {len(x_val)}')\n",
        "print(f'number of y_train examples: {len(y_train)} | number of y_val examples: {len(y_val)}')"
      ],
      "metadata": {
        "id": "MCXLjPoqyhkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What did we just do?\n",
        "- Stripped the original dataset down to area features for our inputs to the network (x) and price labels (y). Our model will learn how area affects prices.\n",
        "\n",
        "- Split our cleaned data into 80% training and 20% validation sets.\n",
        "\n",
        "- Assigned only the house area column in the dataframe to x_train and x_val, and similarly, only the price column to y_train and y_val.\n",
        "\n",
        "- Finally converted x_train/x_val and y_train/y_val to numpy arrays with *.values* so our model can ingest them properly."
      ],
      "metadata": {
        "id": "dFAMQQNVNISr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we work on defining our model, as a final preprocessing step, we need to normalize our data that will be going into the model during training.\n",
        "\n",
        "If we ingest large values for area/prices, our model's computations will be dealing in similarly large values. This can cause numerical instability during training, resulting in extreme and exploding gradients.\n",
        "\n",
        "Let's normalize our training data with z-score normalization according to the formula:\n",
        "$$\n",
        "\\text{X}_{std} = \\frac{X - \\mu}{\\sigma}\n",
        "$$\n",
        "to center our data around the mean at 0, and scale the spread of our data to a standard deviation of 1."
      ],
      "metadata": {
        "id": "O8QWW4SrgOlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'y_train before norm: {y_train}')\n",
        "print(f'y_val before norm: {y_val}')\n",
        "\n",
        "# Compute the means\n",
        "x_mean = np.mean(x_train, axis=0)\n",
        "y_mean = np.mean(y_train, axis=0)\n",
        "x_std = np.std(x_train, axis=0)\n",
        "y_std = np.std(y_train, axis=0)\n",
        "# Standardize our training data\n",
        "x_train = (x_train - x_mean) / x_std\n",
        "y_train = (y_train - y_mean) / y_std\n",
        "\n",
        "# Standardize our validation data\n",
        "# NOTE: we use the std and mean of the *training* data to ensure scaling consistency across both sets\n",
        "# we can't use std or mean of val data for scaling as that leaks info of the val set to the model during training\n",
        "x_val = (x_val - x_mean) / x_std\n",
        "y_val = (y_val - y_mean) / y_std\n",
        "\n",
        "print(f'y_train after norm: {y_train}')\n",
        "print(f'y_val after norm: {y_val}')"
      ],
      "metadata": {
        "id": "3VTRnPangMBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Structure:\n",
        "\n",
        "We want to define our model structure within a class that we can use easily later. We need to tell the class that, upon initialization of a class instance, create weights and bias variables within which we will eventually store and update our model's parameters."
      ],
      "metadata": {
        "id": "JWktrWqIPpio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a class for our model to hold everything\n",
        "class LinearRegressionModel:\n",
        "  def __init__(self):\n",
        "    self.weights = None\n",
        "    self.bias = None"
      ],
      "metadata": {
        "id": "z1EMbrOh4DgG"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have defined how to initialize an instance of our model, we should define how the model will train (also referred to as 'fitting' the data)"
      ],
      "metadata": {
        "id": "8ljr73u0Qrdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a class for our model to hold everything\n",
        "class LinearRegressionModel:\n",
        "  def __init__(self):\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "# train() will take in:\n",
        "# - our training data X_train & y_train\n",
        "# - a parameter 'epochs' for number of training cycles\n",
        "# - a parameter 'learning_rate' to nudge our weights & biases a small amount each cycle\n",
        "# - a parameter 'reg' to penalize large weights\n",
        "\n",
        "  def train(self, x_train, y_train, epochs=100, learning_rate=1e-3, reg=0.01):\n",
        "\n",
        "    num_train, num_features = x_train.shape # use the dimensions of x_train to get number of training examples/features per example\n",
        "\n",
        "    self.weights = np.random.rand(num_features, 1)\n",
        "    self.bias = 0\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # Forward pass - make a prediction\n",
        "      # we use np.dot() as a fast vectorized approach for when we have multiple features\n",
        "      y_pred = np.dot(x_train, self.weights) + self.bias\n",
        "\n",
        "      # Backward pass\n",
        "      # Compute mean-squared-error loss for the training set (see how wrong the prediction was)\n",
        "      train_loss = np.mean((y_train - y_pred)**2) + reg * np.sum(self.weights **2)\n",
        "      train_loss_history.append(train_loss) # save loss each epoch for graphing later\n",
        "\n",
        "      # Compute the gradients that will update our parameters\n",
        "      # Average over the number of training examples and add the derivative of the regularization term\n",
        "      dw = (np.dot(x_train.T, (y_pred - y_train)) / num_train) + (2 * reg * self.weights)\n",
        "      db = np.sum(y_pred - y_train) / num_train\n",
        "\n",
        "      # Nudge the old weights & biases by a small amount, scaled by the current gradients\n",
        "      # these dw,db gradients are the slope of the loss\n",
        "      # the -= is a *negative* accumulation because we want to descend in the direction of the slope!\n",
        "      self.weights -= learning_rate * dw\n",
        "      self.bias -= learning_rate * db\n",
        "\n",
        "      # Compute MSE loss for our validation set to compare\n",
        "      y_pred_val = np.dot(x_val, self.weights) + self.bias\n",
        "      val_loss = np.mean((y_val - y_pred_val)**2) + reg * np.sum(self.weights **2)\n",
        "      val_loss_history.append(val_loss)\n",
        "\n",
        "      # Print loss every 10 epochs\n",
        "      if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}/{epochs} - Train Loss: {train_loss} | Val loss: {val_loss}')\n",
        "\n",
        "    return train_loss_history, val_loss_history # return these so we can plot the loss history graphs\n",
        "\n",
        "  # Define a simple prediction function for test-time\n",
        "  def predict(self, x):\n",
        "    prediction = np.dot(x, self.weights) + self.bias\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "l1BuMWhpQRUG"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our structure and training loop has all been defined, we are ready to hit the big red button and see what it can do!\n",
        "\n",
        "We'll call the model class and assign it to a variable called *model*.\n",
        "\n",
        "Then we'll train the model by feeding it our training data (x_train, y_train) with the return values of train() inside the model being stored in *train_loss_history* and *val_loss_history* so we can see how it does!"
      ],
      "metadata": {
        "id": "VQbbtWVGxYAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for number of epochs (feel free to experiment! more epochs gives the model longer to converge)\n",
        "epochs = 500\n",
        "# How much to nudge the parameters by each epoch (smaller lr will take longer to converge)\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# Regularization parameter\n",
        "# if the train/validation sets are very different, try increasing to prevent overfitting!\n",
        "reg = 0.1\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "train_loss_history, val_loss_history = model.train(x_train, y_train, epochs, learning_rate)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aNCZgo5AZvBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 2 overlaid curves - train loss and val loss\n",
        "plt.plot(train_loss_history, label='Training loss')\n",
        "plt.plot(val_loss_history, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t1KwBUBKkGgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there you have it! Our model has converged which is always a good sign!\n",
        "\n",
        "Though as is evident, our model has a pretty decent sized spread between the train loss and validation loss.\n",
        "\n",
        "Our linear regression model is only learning a function based on one feature - area. There isn't a huge amount of information it can really glean from just one feature, which is why we converge a bit and then don't go much further - it's learned everything it can about the training data.\n",
        "\n",
        "Imagine if you knew a house's area in square feet, but also its age, and the number of rooms and the area.. you could definitely get a better idea of its price than just the area!\n",
        "\n",
        "We need to introduce a few more features in order to add more complexity to the model and hopefully we'll see it glean better insights!\n",
        "\n",
        "When a linear regression model takes in multiple features, it becomes a ***multiple regression model***."
      ],
      "metadata": {
        "id": "HQ5hnu32y6ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Regression"
      ],
      "metadata": {
        "id": "e2lbMEFlPqDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip dataset to columns that now represent prices, bedrooms, bathrooms and areas\n",
        "data_stripped = data.iloc[:,[2,3,4,5]]\n",
        "X = data.iloc[:,[3,4,5]] # assign our model inputs as [bedrooms, bathrooms, area] for each example\n",
        "y = data.iloc[:,[2]] # assign our model labels as prices\n",
        "\n",
        "# 80/15/5 split for train/validation/test sets (we now want a test set to test our model on predictions!)\n",
        "total_examples = len(data)\n",
        "train_split_index = int(total_examples * 0.8)\n",
        "val_split_index = int(total_examples * 0.95)\n",
        "\n",
        "# Assign each split to x,y_train and x,y_val sets & convert to numpy arrays for our model.\n",
        "x_train = X.iloc[:split_index].values\n",
        "y_train = y.iloc[:split_index].values\n",
        "x_val = X[train_split_index:val_split_index]\n",
        "y_val = y[train_split_index:val_split_index]\n",
        "x_test = X[val_split_index:]\n",
        "y_test = y[val_split_index:]\n",
        "\n",
        "# Visualize our data splits\n",
        "print(f'x_test: {x_test}\\n')\n",
        "print(f'y_test: {y_test}\\n')\n",
        "print(f'number of original examples: {len(X)}')\n",
        "print(f'number of x_train examples: {len(x_train)} | number of x_val examples: {len(x_val)}')\n",
        "print(f'number of y_train examples: {len(y_train)} | number of y_val examples: {len(y_val)}')\n",
        "print(f'number of x_test examples: {len(x_test)} | number of y_test examples: {len(y_test)}')"
      ],
      "metadata": {
        "id": "9ikkpVVfz2Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the means & stds\n",
        "x_mean = np.mean(x_train, axis=0)\n",
        "x_std = np.std(x_train, axis=0)\n",
        "y_mean = np.mean(y_train, axis=0)\n",
        "y_std = np.std(y_train, axis=0)\n",
        "\n",
        "# Standardize our training data\n",
        "x_train = (x_train - x_mean) / x_std\n",
        "y_train = (y_train - y_mean) / y_std\n",
        "\n",
        "# Standardize our validation data\n",
        "x_val = (x_val - x_mean) / x_std\n",
        "y_val = (y_val - y_mean) / y_std\n",
        "\n",
        "# Standardize our test data\n",
        "x_test = (x_test - x_mean) / x_std\n",
        "y_test = (y_test - y_mean) / y_std"
      ],
      "metadata": {
        "id": "o7Qxu6xQ2AGi"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for number of epochs (feel free to experiment! more epochs gives the model longer to converge)\n",
        "epochs = 500\n",
        "# How much to nudge the parameters by each epoch (smaller lr will take longer to converge)\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# Regularization parameter - if the train/validation sets are very different, try increasing to prevent overfitting!\n",
        "reg=0.1\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "train_loss_history, val_loss_history = model.train(x_train, y_train, epochs, learning_rate)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A2pdYU3n2dXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 2 overlaid curves - train loss and val loss\n",
        "plt.plot(train_loss_history, label='Training loss')\n",
        "plt.plot(val_loss_history, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5_lf_imH2mp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our model with the test set!\n",
        "y_pred_test = model.predict(x_test)\n",
        "test_error = np.mean((y_test - y_pred_test) **2)\n",
        "print(f'Test error: {test_error}')"
      ],
      "metadata": {
        "id": "yR0Ibdcy4Iqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's about the extent we are going to get into for this notebook.\n",
        "\n",
        "Our model converged, and both train and val sets showed pretty similar curves so we weren't overfitting. This means the model is generalizing well to data it hasn't seen before!\n",
        "\n",
        "We could further improve this model by feature scaling our data. Adding polynomial features or other transformations to the training data can significantly help capture more complexity and get the loss even lower!\n",
        "\n",
        "Hope you had fun and learned something new!"
      ],
      "metadata": {
        "id": "P2N5CszMQVJx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8ekgAUyRPy1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}